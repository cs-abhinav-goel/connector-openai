{
    "name": "openai",
    "version": "1.0.0",
    "label": "OpenAI",
    "description": "This integration supports interacting with OpenAI's powerful language model, ChatGPT from FortiSOAR workflows",
    "publisher": "Fortinet",
    "cs_approved": true,
    "cs_compatible": true,
    "icon_small_name": "small.png",
    "icon_large_name": "large.png",
    "help_online": "https://docs.fortinet.com/document/fortisoar/1.0.0/openai/534/openai-v1-0-0",
    "category": "Miscellaneous",
    "configuration": {
        "fields": [
            {
                "title": "API Key",
                "type": "password",
                "name": "apiKey",
                "description": "Specify the API key to access the endpoint to which you will connect and perform the automated operations",
                "tooltip": "Specify the API key to access the endpoint to which you will connect and perform the automated operations",
                "required": true,
                "visible": true,
                "editable": true,
                "value": null
            }
        ]
    },
    "operations": [
        {
            "title": "Create a chat completion",
            "operation": "chat_completions",
            "annotation": "chat_completions",
            "description": "Creates a completion for a given chat message using a pre-trained deep learning model.",
            "parameters": [
                {
                    "title": "Message",
                    "type": "text",
                    "name": "message",
                    "required": true,
                    "visible": true,
                    "editable": true,
                    "description": "Specify the message for which you want to generate a chat completion.",
                    "tooltip": "Specify the message for which you want to generate a chat completion.",
                    "value": ""
                },
                {
                    "title": "Model",
                    "type": "text",
                    "name": "model",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "value": "gpt-3.5-turbo",
                    "description": "Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo.",
                    "tooltip": "Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo."
                },
                {
                    "title": "Temperature",
                    "type": "text",
                    "name": "temperature",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "description": "Specify sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Generally recommended to use this or Top Probability parameter but not both. By default it is set to 1.",
                    "tooltip": "Specify sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Generally recommended to use this or Top Probability parameter but not both."
                },
                {
                    "title": "Top Probability",
                    "type": "text",
                    "name": "top_p",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Generally recommended to use this or Temperature but not both. By default it is set to 1.",
                    "tooltip": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Generally recommended to use this or Temperature but not both."
                },
                {
                    "title": "Max Tokens",
                    "type": "integer",
                    "name": "max_tokens",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "description": "Specify maximum number of tokens to generate in the chat completion. Note: The total length of input tokens and generated tokens is limited by the model's context length.",
                    "tooltip": "Specify maximum number of tokens to generate in the chat completion. Note: The total length of input tokens and generated tokens is limited by the model's context length."
                }
            ],
            "category": "miscellaneous",
            "output_schema": {
                "id": "",
                "model": "",
                "usage": {
                  "total_tokens": "",
                  "prompt_tokens": "",
                  "completion_tokens": ""
                },
                "object": "",
                "choices": [
                  {
                    "index": "",
                    "message": {
                      "role": "",
                      "content": ""
                    },
                    "finish_reason": ""
                  }
                ],
                "created": ""
            },
            "enabled": true
        }
    ]
}