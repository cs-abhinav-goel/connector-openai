{
   "name":"openai",
   "version":"1.1.0",
   "label":"OpenAI",
   "description":"This integration supports interacting with OpenAI's powerful language model, ChatGPT from FortiSOAR workflows",
   "publisher":"Fortinet",
   "cs_approved":true,
   "cs_compatible":true,
   "icon_small_name":"small.png",
   "icon_large_name":"large.png",
   "help_online":"https://docs.fortinet.com/document/fortisoar/1.1.0/openai/534/openai-v1-1-0",
   "category":"Miscellaneous",
   "configuration":{
      "fields":[
         {
            "title":"API Key",
            "type":"password",
            "name":"apiKey",
            "description":"Specify your API key to authenticate the OpenAI endpoints",
            "tooltip":"Your OpenAI API Key",
            "required":true,
            "visible":true,
            "editable":true,
            "value":null
         },
         {
            "title":"Use Microsoft Azure OpenAI Endpoint",
            "type": "checkbox",
            "name":"api_type",
            "description":"Select if you are using OpenAI services directly, or via a Microsoft Azure Endpoint",
            "tooltip":"Select if you are using OpenAI services directly, or via a Microsoft Azure Endpoint",
            "required":false,
            "visible":true,
            "editable":true,
            "value":false,
            "onchange": {
               "true": [
                  {
                     "title": "Endpoint FQHN",
                     "description": "Specify the Azure OpenAI Endpoint",
                     "tooltip": "Specify the Azure OpenAI Endpoint",
                     "name": "api_base",
                     "type": "text",
                     "visible": true,
                     "editable": true,
                     "required": true,
                     "placeholder": "https://example-endpoint.openai.azure.com"
                   },
                   {
                     "title": "API Version",
                     "description": "Specify the Azure OpenAI API Version",
                     "tooltip": "Specify the Azure OpenAI API Version",
                     "name": "api_version",
                     "type": "text",
                     "visible": true,
                     "editable": true,
                     "required": true,
                     "placeholder": "2023-05-15",
                     "value": "2023-05-15"
                   },
                   {
                     "title": "Deployment Name",
                     "description": "The Azure OpenAI Deployment Name",
                     "tooltip": "The Azure OpenAI Deployment Name",
                     "name": "deployment_id",
                     "type": "text",
                     "visible": true,
                     "editable": true,
                     "required": true
                   }
               ]
            }
            
         }
      ]
   },
   "operations":[
      {
         "title":"Ask a Question",
         "operation":"chat_completions",
         "annotation":"chat_completions",
         "description":"Generates a completion for a given chat message using a pre-trained deep learning model.",
         "parameters":[
            {
               "title":"Message",
               "type":"text",
               "name":"message",
               "required":true,
               "visible":true,
               "editable":true,
               "description":"Specify the message for which you want to generate a chat completion.",
               "tooltip":"Specify the message for which you want to generate a chat completion.",
               "value":""
            },
            {
               "title":"Model",
               "type":"text",
               "name":"model",
               "required":false,
               "visible":true,
               "editable":true,
               "value":"gpt-3.5-turbo",
               "description":"Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo.",
               "tooltip":"Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo."
            },
            {
               "title":"Temperature",
               "type":"text",
               "name":"temperature",
               "required":false,
               "visible":true,
               "editable":true,
               "description":"Specify the sampling temperature between 0 and 2. Higher values like 0.8 makes the output more random, while lower values like makes it more focused and deterministic. NOTE: It is recommended to use either this parameter or Top Probability parameter, not both. By default, it is set to 1.",
               "tooltip":"Specify the sampling temperature between 0 and 2. Higher values like 0.8 makes the output more random, while lower values like makes it more focused and deterministic. NOTE: It is recommended to use either this parameter or Top Probability parameter, not both. By default, it is set to 1."
            },
            {
               "title":"Top Probability",
               "type":"text",
               "name":"top_p",
               "required":false,
               "visible":true,
               "editable":true,
               "description":"Specify the top probability, an alternative to sampling with temperature, also called nucleus sampling. The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. NOTE: It is recommended to use either this parameter or Temperature parameter, not both. By default, it is set to 1.",
               "tooltip":"Specify the top probability, an alternative to sampling with temperature, also called nucleus sampling. The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. NOTE: It is recommended to use either this parameter or Temperature parameter, not both. By default, it is set to 1."
            },
            {
               "title":"Max Tokens",
               "type":"integer",
               "name":"max_tokens",
               "required":false,
               "visible":true,
               "editable":true,
               "description":"Specify maximum number of tokens to generate in the chat completion. NOTE: The total length of input tokens and generated tokens is limited by the model's context length.",
               "tooltip":"Specify maximum number of tokens to generate in the chat completion. NOTE: The total length of input tokens and generated tokens is limited by the model's context length."
            }
         ],
         "category":"miscellaneous",
         "output_schema":{
            "id":"",
            "model":"",
            "usage":{
               "total_tokens":"",
               "prompt_tokens":"",
               "completion_tokens":""
            },
            "object":"",
            "choices":[
               {
                  "index":"",
                  "message":{
                     "role":"",
                     "content":""
                  },
                  "finish_reason":""
               }
            ],
            "created":""
         },
         "enabled":true
      },
      {
         "title":"Converse With OpenAI",
         "operation":"chat_conversation",
         "annotation":"chat_conversation",
         "description":"Ask a question and get the AI answer based on the previous discussion you had.",
         "parameters":[
            {
               "title":"Messages",
               "type":"json",
               "name":"messages",
               "required":true,
               "visible":true,
               "editable":true,
               "description":"Specify the messages list for which you want to generate a chat completion. It should include all previous chat messages as per OpenAI doc",
               "tooltip":"Specify the messages list for which you want to generate a chat completion. It should include all previous chat messages.",
               "value":"[{\"role\": \"user\", \"content\": \"when was stuxnet first seen\"},{\"role\": \"assistant\", \"content\": \"Stuxnet was first identified by the infosec community in 2010, but development on it probably began in 20051. I hope this helps!\"},{\"role\": \"user\", \"content\": \"who discovered it\"}]"
            },
            {
               "title":"Model",
               "type":"text",
               "name":"model",
               "required":false,
               "visible":true,
               "editable":true,
               "value":"gpt-3.5-turbo",
               "description":"Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo.",
               "tooltip":"Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo."
            },
            {
               "title":"Temperature",
               "type":"text",
               "name":"temperature",
               "required":false,
               "visible":true,
               "editable":true,
               "description":"Specify the sampling temperature between 0 and 2. Higher values like 0.8 makes the output more random, while lower values like makes it more focused and deterministic. NOTE: It is recommended to use either this parameter or Top Probability parameter, not both. By default, it is set to 1.",
               "tooltip":"Specify the sampling temperature between 0 and 2. Higher values like 0.8 makes the output more random, while lower values like makes it more focused and deterministic. NOTE: It is recommended to use either this parameter or Top Probability parameter, not both. By default, it is set to 1."
            },
            {
               "title":"Top Probability",
               "type":"text",
               "name":"top_p",
               "required":false,
               "visible":true,
               "editable":true,
               "description":"Specify the top probability, an alternative to sampling with temperature, also called nucleus sampling. The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. NOTE: It is recommended to use either this parameter or Temperature parameter, not both. By default, it is set to 1.",
               "tooltip":"Specify the top probability, an alternative to sampling with temperature, also called nucleus sampling. The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. NOTE: It is recommended to use either this parameter or Temperature parameter, not both. By default, it is set to 1."
            },
            {
               "title":"Max Tokens",
               "type":"integer",
               "name":"max_tokens",
               "required":false,
               "visible":true,
               "editable":true,
               "description":"Specify maximum number of tokens to generate in the chat completion. NOTE: The total length of input tokens and generated tokens is limited by the model's context length.",
               "tooltip":"Specify maximum number of tokens to generate in the chat completion. NOTE: The total length of input tokens and generated tokens is limited by the model's context length."
            }
         ],
         "category":"miscellaneous",
         "output_schema":{
            "id":"",
            "model":"",
            "usage":{
               "total_tokens":"",
               "prompt_tokens":"",
               "completion_tokens":""
            },
            "object":"",
            "choices":[
               {
                  "index":"",
                  "message":{
                     "role":"",
                     "content":""
                  },
                  "finish_reason":""
               }
            ],
            "created":""
         },
         "enabled":true
      },
      {
         "operation":"list_models",
         "title":"List Available Models",
         "description":"Lists and describe the various models available in the API.",
         "category":"miscellaneous",
         "annotation":"list_models",
         "enabled":true,
         "parameters":[
            
         ],
         "output_schema":{
            "data":[
               {
                  "id":"",
                  "object":"",
                  "created":"",
                  "owned_by":""
               }
            ],
            "object":""
         }
      },
      {
         "operation":"get_usage",
         "title":"Get Tokens Usage",
         "description":"Lists the usage for each call for a specific date",
         "category":"miscellaneous",
         "annotation":"get_usage",
         "enabled":true,
         "parameters":[
            {
               "title":"Date",
               "type":"datetime",
               "name":"date",
               "required":true,
               "visible":true,
               "editable":true,
               "description":"Date to fetch the report for.",
               "tooltip":"Date to fetch the report for.",
               "value":""
            }
         ],
         "output_schema":{
            "object":"",
            "data":[
               {
                  "aggregation_timestamp":"",
                  "n_requests":"",
                  "operation":"",
                  "snapshot_id":"",
                  "n_context":"",
                  "n_context_tokens_total":"",
                  "n_generated":"",
                  "n_generated_tokens_total":""
               }
            ],
            "ft_data":"",
            "dalle_api_data":"",
            "whisper_api_data":"",
            "current_usage_usd":""
         }
      }
   ]
}