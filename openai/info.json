{
    "name": "openai",
    "version": "1.0.0",
    "label": "OpenAI",
    "description": "This integration supports interacting with OpenAI's powerful language model, ChatGPT from FortiSOAR workflows",
    "publisher": "Fortinet",
    "cs_approved": true,
    "cs_compatible": true,
    "icon_small_name": "small.png",
    "icon_large_name": "large.png",
    "help_online": "https://docs.fortinet.com/document/fortisoar/1.0.0/openai/534/openai-v1-0-0",
    "category": "Miscellaneous",
    "configuration": {
        "fields": [
            {
                "title": "API Key",
                "type": "password",
                "name": "apiKey",
                "description": "Specify the API key to access the endpoint to which you will connect and perform the automated operations",
                "tooltip": "Specify the API key to access the endpoint to which you will connect and perform the automated operations",
                "required": true,
                "visible": true,
                "editable": true,
                "value": null
            }
        ]
    },
    "operations": [
        {
            "title": "Create a chat completion",
            "operation": "chat_completions",
            "annotation": "chat_completions",
            "description": "Generates a completion for a given chat message using a pre-trained deep learning model.",
            "parameters": [
                {
                    "title": "Message",
                    "type": "text",
                    "name": "message",
                    "required": true,
                    "visible": true,
                    "editable": true,
                    "description": "Specify the message for which you want to generate a chat completion.",
                    "tooltip": "Specify the message for which you want to generate a chat completion.",
                    "value": ""
                },
                {
                    "title": "Model",
                    "type": "text",
                    "name": "model",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "value": "gpt-3.5-turbo",
                    "description": "Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo.",
                    "tooltip": "Specify the ID of the GPT model to use for the chat completion. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported. By default it is set to gpt-3.5-turbo."
                },
                {
                    "title": "Temperature",
                    "type": "text",
                    "name": "temperature",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "description": "Specify the sampling temperature between 0 and 2. Higher values like 0.8 makes the output more random, while lower values like makes it more focused and deterministic. NOTE: It is recommended to use either this parameter or Top Probability parameter, not both. By default, it is set to 1.",
                    "tooltip": "Specify the sampling temperature between 0 and 2. Higher values like 0.8 makes the output more random, while lower values like makes it more focused and deterministic. NOTE: It is recommended to use either this parameter or Top Probability parameter, not both. By default, it is set to 1."
                },
                {
                    "title": "Top Probability",
                    "type": "text",
                    "name": "top_p",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "description": "Specify the top probability, an alternative to sampling with temperature, also called nucleus sampling. The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. NOTE: It is recommended to use either this parameter or Temperature parameter, not both. By default, it is set to 1.",
                    "tooltip": "Specify the top probability, an alternative to sampling with temperature, also called nucleus sampling. The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. NOTE: It is recommended to use either this parameter or Temperature parameter, not both. By default, it is set to 1."
                },
                {
                    "title": "Max Tokens",
                    "type": "integer",
                    "name": "max_tokens",
                    "required": false,
                    "visible": true,
                    "editable": true,
                    "description": "Specify maximum number of tokens to generate in the chat completion. NOTE: The total length of input tokens and generated tokens is limited by the model's context length.",
                    "tooltip": "Specify maximum number of tokens to generate in the chat completion. NOTE: The total length of input tokens and generated tokens is limited by the model's context length."
                }
            ],
            "category": "miscellaneous",
            "output_schema": {
                "id": "",
                "model": "",
                "usage": {
                  "total_tokens": "",
                  "prompt_tokens": "",
                  "completion_tokens": ""
                },
                "object": "",
                "choices": [
                  {
                    "index": "",
                    "message": {
                      "role": "",
                      "content": ""
                    },
                    "finish_reason": ""
                  }
                ],
                "created": ""
            },
            "enabled": true
        }
    ]
}